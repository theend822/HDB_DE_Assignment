{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDB Technical Test for Senior Data Engineer — Code Execution\n",
    "\n",
    "End-to-end pipeline: Download → Merge → DQC → Separate valid/failed → Transform\n",
    "\n",
    "All results are saved to the corresponding `data/` subfolders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/jxy/Desktop/500k/HDB/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE_URL = \"https://data.gov.sg/api/action/datastore_search\"\n",
    "API_KEY = 'v2:bb4763c933c4126263a5dd2c70c0a8918fddbdbae474b1129924057961fc4e4a:sNbOsXx5fkHYP5OmBDUWSuesAdWGb6-M'\n",
    "headers = {\"x-api-key\": API_KEY} if API_KEY else {}\n",
    "\n",
    "DATASET_ID = {\n",
    "    \"1990_1999\":    \"d_ebc5ab87086db484f88045b47411ebc5\",\n",
    "    \"2000_2012feb\": \"d_43f493c6c50d54243cc1eab0df142d6a\",\n",
    "    \"2012mar_2014\": \"d_2d5ff9ea31397b66239f245f57751537\",\n",
    "    \"2015_2016\":    \"d_ea9ed51da2787afaf8e51f827c304208\",\n",
    "    \"2017_onwards\": \"d_8b84c4ee58e3cfc0ece0d773c8ca6abc\",\n",
    "}\n",
    "\n",
    "RAW_DATA_DIR    = \"../data/raw\"\n",
    "STAGE_DATA_DIR  = \"../data/stage\"\n",
    "PROD_DATA_DIR   = \"../data/prod\"\n",
    "FAILED_DATA_DIR = \"../data/failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def download(resource_id, limit=10000, sleep_secs=12):\n",
    "    \"\"\"\n",
    "    Fetch all records for a resource_id via paginated API calls.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    offset  = 0\n",
    "\n",
    "    while True:\n",
    "        url = f\"{API_BASE_URL}?resource_id={resource_id}&limit={limit}&offset={offset}\"\n",
    "\n",
    "        data  = requests.get(url, headers=headers, timeout=60).json()\n",
    "        batch = data[\"result\"][\"records\"]\n",
    "        total = data[\"result\"].get(\"total\", 0)\n",
    "\n",
    "        if not batch:\n",
    "            break\n",
    "\n",
    "        records.extend(batch)\n",
    "        offset += len(batch)\n",
    "        print(f\"  fetched {offset:>7,} / {total:,}\", end=\"\\r\")\n",
    "\n",
    "        if offset >= total:\n",
    "            break\n",
    "\n",
    "        time.sleep(sleep_secs)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Save data into raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1990_1999...\n",
      "  done: 287,196 rows  → ../data/raw/1990_1999.csv\n",
      "Downloading 2000_2012feb...\n",
      "  done: 369,651 rows  → ../data/raw/2000_2012feb.csv\n",
      "Downloading 2012mar_2014...\n",
      "  done:  52,203 rows  → ../data/raw/2012mar_2014.csv\n",
      "Downloading 2015_2016...\n",
      "  done:  37,153 rows  → ../data/raw/2015_2016.csv\n",
      "Downloading 2017_onwards...\n",
      "  done: 225,421 rows  → ../data/raw/2017_onwards.csv\n"
     ]
    }
   ],
   "source": [
    "raw_dfs = {}\n",
    "for name, resource_id in DATASET_ID.items():\n",
    "    print(f\"Downloading {name}...\")\n",
    "    df = download(resource_id)\n",
    "    raw_dfs[name] = df\n",
    "\n",
    "    out_path = f\"{RAW_DATA_DIR}/{name}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"  done: {len(df):>7,} rows  → {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped _id from 1990_1999\n",
      "Dropped _id from 2000_2012feb\n",
      "Dropped _id from 2012mar_2014\n",
      "Dropped _id from 2015_2016\n",
      "Dropped _id from 2017_onwards\n"
     ]
    }
   ],
   "source": [
    "# drop _id column\n",
    "for name in raw_dfs:\n",
    "    if \"_id\" in raw_dfs[name].columns:\n",
    "        raw_dfs[name] = raw_dfs[name].drop(columns=\"_id\")\n",
    "        print(f\"Dropped _id from {name}\")\n",
    "\n",
    "# drop remaining_lease as it will be recalculated uniformly in transform step\n",
    "for name in [\"2015_2016\", \"2017_onwards\"]:\n",
    "    if \"remaining_lease\" in raw_dfs[name].columns:\n",
    "        raw_dfs[name] = raw_dfs[name].drop(columns=[\"remaining_lease\"])\n",
    "        print(f\"Dropped remaining_lease from {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990_1999      : ['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price']\n",
      "2000_2012feb   : ['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price']\n",
      "2012mar_2014   : ['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price']\n",
      "2015_2016      : ['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price']\n",
      "2017_onwards   : ['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price']\n"
     ]
    }
   ],
   "source": [
    "# Verify all datasets now have the same columns\n",
    "for name, rdf in raw_dfs.items():\n",
    "    print(f\"{name:15s}: {list(rdf.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge 5 dataset and Save into raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged: 971,624 rows, 10 columns\n",
      "Saved → ../data/raw/merged_raw.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(raw_dfs.values(), ignore_index=True, sort=False)\n",
    "print(f\"Merged: {len(df):,} rows, {df.shape[1]} columns\")\n",
    "\n",
    "merged_path = f\"{RAW_DATA_DIR}/merged_raw.csv\"\n",
    "df.to_csv(merged_path, index=False)\n",
    "print(f\"Saved → {merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>town</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>block</th>\n",
       "      <th>street_name</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>flat_model</th>\n",
       "      <th>lease_commence_date</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>309</td>\n",
       "      <td>ANG MO KIO AVE 1</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>31</td>\n",
       "      <td>IMPROVED</td>\n",
       "      <td>1977</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>309</td>\n",
       "      <td>ANG MO KIO AVE 1</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>31</td>\n",
       "      <td>IMPROVED</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1990-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>309</td>\n",
       "      <td>ANG MO KIO AVE 1</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>31</td>\n",
       "      <td>IMPROVED</td>\n",
       "      <td>1977</td>\n",
       "      <td>8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>1 ROOM</td>\n",
       "      <td>309</td>\n",
       "      <td>ANG MO KIO AVE 1</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>31</td>\n",
       "      <td>IMPROVED</td>\n",
       "      <td>1977</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990-01</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>216</td>\n",
       "      <td>ANG MO KIO AVE 1</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>73</td>\n",
       "      <td>NEW GENERATION</td>\n",
       "      <td>1976</td>\n",
       "      <td>47200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month        town flat_type block       street_name storey_range  \\\n",
       "0  1990-01  ANG MO KIO    1 ROOM   309  ANG MO KIO AVE 1     10 TO 12   \n",
       "1  1990-01  ANG MO KIO    1 ROOM   309  ANG MO KIO AVE 1     04 TO 06   \n",
       "2  1990-01  ANG MO KIO    1 ROOM   309  ANG MO KIO AVE 1     10 TO 12   \n",
       "3  1990-01  ANG MO KIO    1 ROOM   309  ANG MO KIO AVE 1     07 TO 09   \n",
       "4  1990-01  ANG MO KIO    3 ROOM   216  ANG MO KIO AVE 1     04 TO 06   \n",
       "\n",
       "  floor_area_sqm      flat_model lease_commence_date resale_price  \n",
       "0             31        IMPROVED                1977         9000  \n",
       "1             31        IMPROVED                1977         6000  \n",
       "2             31        IMPROVED                1977         8000  \n",
       "3             31        IMPROVED                1977         6000  \n",
       "4             73  NEW GENERATION                1976        47200  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DQC — same checks as the DAG\n",
    "\n",
    "Each check function returns a boolean mask (True = row failed).\n",
    "We also write a 0/1 result file per check to `data/stage/dqc_results/` — exactly what the production DAG does, so `separate_valid_failed()` can consume them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DQC Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQ_CHECKS = {\n",
    "    \"null\": [\n",
    "        \"month\", \"town\", \"flat_type\", \"block\", \"street_name\",\n",
    "        \"storey_range\", \"floor_area\", \"flat_model\",\n",
    "        \"lease_commence_date\", \"resale_price\",\n",
    "    ],\n",
    "    \"categorical\": {\n",
    "        \"town\": {\"allowed_values\": [\n",
    "            \"ANG MO KIO\", \"BEDOK\", \"BISHAN\", \"BUKIT BATOK\", \"BUKIT MERAH\",\n",
    "            \"BUKIT PANJANG\", \"BUKIT TIMAH\", \"CENTRAL AREA\", \"CHOA CHU KANG\",\n",
    "            \"CLEMENTI\", \"GEYLANG\", \"HOUGANG\", \"JURONG EAST\", \"JURONG WEST\",\n",
    "            \"KALLANG/WHAMPOA\", \"LIM CHU KANG\", \"MARINE PARADE\", \"PASIR RIS\",\n",
    "            \"QUEENSTOWN\", \"SEMBAWANG\", \"SENGKANG\", \"SERANGOON\", \"TAMPINES\",\n",
    "            \"TOA PAYOH\", \"WOODLANDS\", \"YISHUN\",\n",
    "        ]},\n",
    "        \"flat_type\": {\"allowed_values\": [\n",
    "            \"1 ROOM\", \"2 ROOM\", \"3 ROOM\", \"4 ROOM\", \"5 ROOM\",\n",
    "            \"EXECUTIVE\", \"MULTI-GENERATION\",\n",
    "        ]},\n",
    "        \"flat_model\": {\"allowed_values\": [\n",
    "            \"2-ROOM\", \"APARTMENT\", \"IMPROVED\", \"IMPROVED-MAISONETTE\",\n",
    "            \"MAISONETTE\", \"MODEL A\", \"MODEL A-MAISONETTE\", \"MULTI GENERATION\",\n",
    "            \"NEW GENERATION\", \"PREMIUM APARTMENT\", \"SIMPLIFIED\",\n",
    "            \"STANDARD\", \"TERRACE\",\n",
    "        ]},\n",
    "    },\n",
    "    \"string_format\": {\n",
    "        \"storey_range\": {\"pattern\": r\"^\\d{2} TO \\d{2}$\"},\n",
    "    },\n",
    "    \"date_format\": {\n",
    "        \"month\":               {\"fmt\": \"%Y-%m\"},\n",
    "        \"lease_commence_date\": {\"fmt\": \"%Y\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "DUPLICATE_CHECK = {\"key_columns\": None}\n",
    "\n",
    "RESALE_PRICE_OUTLIER_CHECK = {\n",
    "    \"column\":        \"resale_price\",\n",
    "    \"threshold_pct\": 0.20,\n",
    "    \"group_by\":      [\"month\", \"flat_type\", \"block\", \"street_name\", \"storey_range\", \"floor_area\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DQC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check functions — mirrors data_operations/validate.py\n",
    "\n",
    "def check_null(df, column):\n",
    "    return df[column].isna()\n",
    "\n",
    "def check_categorical(df, column, allowed_values):\n",
    "    return ~df[column].isin(allowed_values)\n",
    "\n",
    "def check_string_format(df, column, pattern):\n",
    "    return ~df[column].astype(str).str.match(pattern, na=False)\n",
    "\n",
    "def check_date_format(df, column, fmt):\n",
    "    return pd.to_datetime(df[column].astype(str), format=fmt, errors=\"coerce\").isna()\n",
    "\n",
    "def check_duplicates(df, key_columns=None):\n",
    "    if key_columns is None:\n",
    "        key_columns = [c for c in df.columns if c != \"resale_price\"]\n",
    "    df_sorted = df.sort_values(\"resale_price\", ascending=False)\n",
    "    keep_mask = ~df_sorted.duplicated(subset=key_columns, keep=\"first\")\n",
    "    return ~keep_mask.sort_index()\n",
    "\n",
    "def check_resale_price_outlier(df, column, threshold_pct, group_by):\n",
    "    group_mean = df.groupby(group_by)[column].transform(\"mean\")\n",
    "    return ~df[column].between(group_mean * (1 - threshold_pct), group_mean * (1 + threshold_pct))\n",
    "\n",
    "print(\"Check functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQC Group 1 — null, categorical, string_format, date_format\n",
    "# Each check saves its 0/1 result to dqc_results/ AND accumulates into fail_sum\n",
    "\n",
    "df[\"fail_sum\"] = 0\n",
    "\n",
    "for check_type, check_config in DQ_CHECKS.items():\n",
    "    items = [(col, {}) for col in check_config] if isinstance(check_config, list) else check_config.items()\n",
    "    for column, params in items:\n",
    "        if check_type == \"null\":\n",
    "            mask = check_null(df, column)\n",
    "        elif check_type == \"categorical\":\n",
    "            mask = check_categorical(df, column, **params)\n",
    "        elif check_type == \"string_format\":\n",
    "            mask = check_string_format(df, column, **params)\n",
    "        elif check_type == \"date_format\":\n",
    "            mask = check_date_format(df, column, **params)\n",
    "\n",
    "        # Save result file (mirrors production DAG)\n",
    "        result_file = f\"{DQC_RESULTS_DIR}/{check_type}__{column}.csv\"\n",
    "        mask.astype(int).to_csv(result_file, index=True, header=True)\n",
    "\n",
    "        fails = mask.sum()\n",
    "        df[\"fail_sum\"] += mask.astype(int)\n",
    "        print(f\"{check_type:15s} | {column:25s} | {fails:6,} fails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQC Group 2 — duplicates\n",
    "mask = check_duplicates(df, **DUPLICATE_CHECK)\n",
    "\n",
    "mask.astype(int).to_csv(f\"{DQC_RESULTS_DIR}/duplicates.csv\", index=True, header=True)\n",
    "\n",
    "dup_fails = mask.sum()\n",
    "df[\"fail_sum\"] += mask.astype(int)\n",
    "print(f\"Duplicates: {dup_fails:,} rows flagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQC Group 3 — resale price outlier\n",
    "df[\"resale_price\"] = pd.to_numeric(df[\"resale_price\"], errors=\"coerce\")\n",
    "\n",
    "mask = check_resale_price_outlier(df, **RESALE_PRICE_OUTLIER_CHECK)\n",
    "\n",
    "mask.astype(int).to_csv(f\"{DQC_RESULTS_DIR}/resale_price_outlier.csv\", index=True, header=True)\n",
    "\n",
    "outlier_fails = mask.sum()\n",
    "df[\"fail_sum\"] += mask.astype(int)\n",
    "print(f\"Price outliers: {outlier_fails:,} rows flagged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQC summary\n",
    "print(f\"Total rows:      {len(df):,}\")\n",
    "print(f\"Rows with fails: {(df['fail_sum'] > 0).sum():,}\")\n",
    "print(f\"\\nfail_sum distribution:\")\n",
    "print(df[\"fail_sum\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Separate Valid / Non-valid\n",
    "\n",
    "Rows with `fail_sum > 0` failed at least one check.\n",
    "Saved to `data/stage/validated.csv` and `data/failed/non_valid_records.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid     = df[df[\"fail_sum\"] == 0].drop(columns=[\"fail_sum\"]).reset_index(drop=True)\n",
    "df_non_valid = df[df[\"fail_sum\"] > 0].reset_index(drop=True)\n",
    "\n",
    "validated_path  = f\"{STAGE_DATA_DIR}/validated.csv\"\n",
    "non_valid_path  = f\"{FAILED_DATA_DIR}/non_valid_records.csv\"\n",
    "\n",
    "df_valid.to_csv(validated_path, index=False)\n",
    "df_non_valid.to_csv(non_valid_path, index=False)\n",
    "\n",
    "print(f\"Valid:     {len(df_valid):,}  → {validated_path}\")\n",
    "print(f\"Non-valid: {len(df_non_valid):,}  → {non_valid_path}\")\n",
    "\n",
    "df_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect non-valid rows\n",
    "print(\"Sample non-valid rows:\")\n",
    "df_non_valid.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform\n",
    "\n",
    "Three transformations applied to valid rows only:\n",
    "1. **Remaining lease** — years + months left on the 99-year HDB lease\n",
    "2. **Resale identifier** — a short coded ID per transaction\n",
    "3. **Hash identifier** — SHA-256 hash of the resale identifier\n",
    "\n",
    "Results saved to `data/prod/transformed.csv` (with identifier, without hash) and `data/prod/hashed.csv` (with hash, without identifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 1: Remaining lease\n",
    "reference_date = datetime.now()\n",
    "\n",
    "df_valid[\"remaining_lease\"] = df_valid[\"lease_commence_date\"].apply(\n",
    "    lambda y: \"{} years {} months\".format(\n",
    "        max(relativedelta(datetime(int(y), 1, 1) + relativedelta(years=99), reference_date).years, 0),\n",
    "        max(relativedelta(datetime(int(y), 1, 1) + relativedelta(years=99), reference_date).months, 0),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Sample remaining_lease values:\")\n",
    "df_valid[[\"lease_commence_date\", \"remaining_lease\"]].drop_duplicates().sort_values(\"lease_commence_date\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 2: Resale identifier\n",
    "# Format: S + block_digits(3) + avg_price_digits(2) + month_digits(2) + town_initial\n",
    "df_valid[\"resale_price\"] = pd.to_numeric(df_valid[\"resale_price\"])\n",
    "\n",
    "avg = df_valid.groupby([\"month\", \"town\", \"flat_type\"])[\"resale_price\"].mean().rename(\"avg_price\")\n",
    "df_valid = df_valid.join(avg, on=[\"month\", \"town\", \"flat_type\"])\n",
    "\n",
    "block_digits = (\n",
    "    df_valid[\"block\"].astype(str)\n",
    "    .str.replace(r\"\\D\", \"\", regex=True)\n",
    "    .str[:3].str.zfill(3)\n",
    ")\n",
    "price_digits = df_valid[\"avg_price\"].astype(int).astype(str).str[:2].str.zfill(2)\n",
    "month_digits = pd.to_datetime(df_valid[\"month\"], format=\"%Y-%m\").dt.strftime(\"%m\")\n",
    "town_char    = df_valid[\"town\"].str.strip().str[0].str.upper()\n",
    "\n",
    "df_valid[\"resale_identifier\"] = \"S\" + block_digits + price_digits + month_digits + town_char\n",
    "df_valid = df_valid.drop(columns=[\"avg_price\"])\n",
    "\n",
    "print(f\"Unique identifiers: {df_valid['resale_identifier'].nunique():,} / {len(df_valid):,} rows\")\n",
    "df_valid[[\"block\", \"month\", \"town\", \"resale_price\", \"resale_identifier\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 3: Hash identifier\n",
    "df_valid[\"resale_identifier_hash\"] = df_valid[\"resale_identifier\"].apply(\n",
    "    lambda x: hashlib.sha256(x.encode()).hexdigest()\n",
    ")\n",
    "\n",
    "print(f\"Unique hashes: {df_valid['resale_identifier_hash'].nunique():,}\")\n",
    "df_valid[[\"resale_identifier\", \"resale_identifier_hash\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final outputs\n",
    "df_transformed = df_valid.drop(columns=[\"resale_identifier_hash\"])\n",
    "df_hashed      = df_valid.drop(columns=[\"resale_identifier\"])\n",
    "\n",
    "transformed_path = f\"{PROD_DATA_DIR}/transformed.csv\"\n",
    "hashed_path      = f\"{PROD_DATA_DIR}/hashed.csv\"\n",
    "\n",
    "df_transformed.to_csv(transformed_path, index=False)\n",
    "df_hashed.to_csv(hashed_path, index=False)\n",
    "\n",
    "print(f\"transformed: {df_transformed.shape}  → {transformed_path}\")\n",
    "print(f\"hashed:      {df_hashed.shape}  → {hashed_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
