{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDB Resale Prices — Data Exploration\n",
    "\n",
    "Mirrors the ETL pipeline flow. Code here is prototyped first, then migrated to `data_operations/`.\n",
    "\n",
    "**Flow:** Download → Merge → DQC → Separate valid/failed → Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport requests\nimport pandas as pd\nimport hashlib\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\nfrom config.CONFIG_hdb_resales_price import API_BASE_URL, DATASET_ID, RAW_DATA_DIR\nfrom config.DQC_hdb_resales_price import DQ_CHECKS, DUPLICATE_CHECK, RESALE_PRICE_OUTLIER_CHECK"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Download Data\nSame approach as `fetch_and_save_from_api()` — one call per dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def download(resource_id):\n    url = f\"{API_BASE_URL}?resource_id={resource_id}\"\n    data = requests.get(url, timeout=30).json()\n    return pd.DataFrame(data[\"result\"][\"records\"])\n\n# Download all datasets\nraw_dfs = {name: download(rid) for name, rid in DATASET_ID.items()}\n\nfor name, df in raw_dfs.items():\n    print(f\"{name:15s}  {len(df):>7,} rows  cols={list(df.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Merge\n",
    "Same as `merge_raw_files()` — concat all datasets, keep all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(raw_dfs.values(), ignore_index=True, sort=False)\n",
    "print(f\"Merged: {len(df):,} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. DQC — same checks as the DAG\n\nEach check returns a boolean mask (True = row failed). We accumulate a `fail_sum` per row.\nIn production each check runs as a separate Airflow task writing its 0/1 result to its own file."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check functions (mirrors data_operations/validate.py)\n\ndef check_null(df, column):\n    return df[column].isna()\n\ndef check_categorical(df, column, allowed_values):\n    return ~df[column].isin(allowed_values)\n\ndef check_string_format(df, column, pattern):\n    return ~df[column].astype(str).str.match(pattern, na=False)\n\ndef check_date_format(df, column, fmt):\n    return pd.to_datetime(df[column].astype(str), format=fmt, errors=\"coerce\").isna()\n\ndef check_duplicates(df, key_columns=None):\n    if key_columns is None:\n        key_columns = [c for c in df.columns if c != \"resale_price\"]\n    df_sorted = df.sort_values(\"resale_price\", ascending=False)\n    keep_mask = ~df_sorted.duplicated(subset=key_columns, keep=\"first\")\n    return (~keep_mask.sort_index())\n\ndef check_resale_price_outlier(df, column, threshold_pct, group_by):\n    group_mean = df.groupby(group_by)[column].transform(\"mean\")\n    return ~df[column].between(group_mean * (1 - threshold_pct), group_mean * (1 + threshold_pct))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DQC Group 1: run all DQ_CHECKS from config\n# In the DAG each (check_type, column) pair becomes one task\n\ndf[\"fail_sum\"] = 0\n\nfor check_type, check_config in DQ_CHECKS.items():\n    items = [(col, {}) for col in check_config] if isinstance(check_config, list) else check_config.items()\n    for column, params in items:\n        if check_type == \"null\":\n            mask = check_null(df, column)\n        elif check_type == \"categorical\":\n            mask = check_categorical(df, column, **params)\n        elif check_type == \"string_format\":\n            mask = check_string_format(df, column, **params)\n        elif check_type == \"date_format\":\n            mask = check_date_format(df, column, **params)\n\n        fails = mask.sum()\n        df[\"fail_sum\"] += mask.astype(int)\n        print(f\"{check_type:15s} | {column:25s} | {fails:6,} fails\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DQC Group 2: duplicates\nmask = check_duplicates(df, **DUPLICATE_CHECK)\ndup_fails = mask.sum()\ndf[\"fail_sum\"] += mask.astype(int)\nprint(f\"Duplicates: {dup_fails:,} rows flagged\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DQC Group 3: resale price outlier\ndf[\"resale_price\"] = pd.to_numeric(df[\"resale_price\"], errors=\"coerce\")\n\nmask = check_resale_price_outlier(df, **RESALE_PRICE_OUTLIER_CHECK)\noutlier_fails = mask.sum()\ndf[\"fail_sum\"] += mask.astype(int)\nprint(f\"Price outliers: {outlier_fails:,} rows flagged\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Separate Valid / Non-valid\nSame as `separate_valid_failed()` — rows with `fail_sum > 0` failed at least one check."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_valid     = df[df[\"fail_sum\"] == 0].drop(columns=[\"fail_sum\"])\ndf_non_valid = df[df[\"fail_sum\"] > 0]\n\nprint(f\"Valid:     {len(df_valid):,}\")\nprint(f\"Non-valid: {len(df_non_valid):,}\")\n\ndf_valid.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect non-valid rows\ndf_non_valid.head(20)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transform\n",
    "Same three transformations as `data_operations/transform.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transformation 1: Remaining lease\nreference_date = datetime.now()\n\ndf_valid[\"remaining_lease\"] = df_valid[\"lease_commence_date\"].apply(\n    lambda y: \"{} years {} months\".format(\n        max(relativedelta(datetime(int(y), 1, 1) + relativedelta(years=99), reference_date).years, 0),\n        max(relativedelta(datetime(int(y), 1, 1) + relativedelta(years=99), reference_date).months, 0),\n    )\n)\ndf_valid[[\"lease_commence_date\", \"remaining_lease\"]].head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 2: Resale identifier\n",
    "df_valid[\"resale_price\"] = pd.to_numeric(df_valid[\"resale_price\"])\n",
    "\n",
    "avg = df_valid.groupby([\"month\", \"town\", \"flat_type\"])[\"resale_price\"].mean().rename(\"avg_price\")\n",
    "df_valid = df_valid.join(avg, on=[\"month\", \"town\", \"flat_type\"])\n",
    "\n",
    "block_digits = df_valid[\"block\"].astype(str).str.replace(r\"\\D\", \"\", regex=True).str[:3].str.zfill(3)\n",
    "price_digits = df_valid[\"avg_price\"].astype(int).astype(str).str[:2].str.zfill(2)\n",
    "month_digits = pd.to_datetime(df_valid[\"month\"], format=\"%Y-%m\").dt.strftime(\"%m\")\n",
    "town_char    = df_valid[\"town\"].str.strip().str[0].str.upper()\n",
    "\n",
    "df_valid[\"resale_identifier\"] = \"S\" + block_digits + price_digits + month_digits + town_char\n",
    "df_valid = df_valid.drop(columns=[\"avg_price\"])\n",
    "\n",
    "print(f\"Unique identifiers: {df_valid['resale_identifier'].nunique():,} / {len(df_valid):,} rows\")\n",
    "df_valid[[\"block\", \"month\", \"town\", \"resale_price\", \"resale_identifier\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 3: Hash identifier\n",
    "df_valid[\"resale_identifier_hash\"] = df_valid[\"resale_identifier\"].apply(\n",
    "    lambda x: hashlib.sha256(x.encode()).hexdigest()\n",
    ")\n",
    "\n",
    "print(f\"Unique hashes: {df_valid['resale_identifier_hash'].nunique():,}\")\n",
    "df_valid[[\"resale_identifier\", \"resale_identifier_hash\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final outputs\n",
    "df_transformed = df_valid.drop(columns=[\"resale_identifier_hash\"])\n",
    "df_hashed      = df_valid.drop(columns=[\"resale_identifier\"])\n",
    "\n",
    "print(\"transformed:\", df_transformed.shape)\n",
    "print(\"hashed:     \", df_hashed.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}