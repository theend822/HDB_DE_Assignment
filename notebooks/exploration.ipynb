{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDB Resale Flat Prices - Data Exploration\n",
    "\n",
    "This notebook documents the exploratory data analysis process that informed the ETL pipeline design.\n",
    "\n",
    "**Exploration Flow:**\n",
    "1. Download data from API\n",
    "2. Explore data structure and quality\n",
    "3. Profile columns to understand distributions\n",
    "4. Design and test validation rules\n",
    "5. Develop and test transformations\n",
    "6. Migrate working code to `data_operations/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Data from API\n",
    "\n",
    "Testing API connection and data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API configuration\n",
    "API_BASE_URL = \"https://data.gov.sg/api/action/datastore_search\"\n",
    "RESOURCE_IDS = [\n",
    "    \"1b702208-44bf-4829-b620-4615ee19b57c\",  # 2012-2014\n",
    "    \"83b2fc37-ce8c-4df4-968b-370fd818138b\",  # 2015-2016\n",
    "]\n",
    "\n",
    "print(f\"Will fetch from {len(RESOURCE_IDS)} resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fetching from first resource (limited records)\n",
    "test_params = {\n",
    "    \"resource_id\": RESOURCE_IDS[0],\n",
    "    \"limit\": 1000,\n",
    "    \"offset\": 0\n",
    "}\n",
    "\n",
    "response = requests.get(API_BASE_URL, params=test_params, timeout=30)\n",
    "data = response.json()\n",
    "\n",
    "if data.get(\"success\"):\n",
    "    records = data[\"result\"][\"records\"]\n",
    "    df_test = pd.DataFrame(records)\n",
    "    print(f\"✓ Fetched {len(df_test)} records successfully\")\n",
    "    print(f\"\\nColumns: {list(df_test.columns)}\")\n",
    "    df_test.head()\n",
    "else:\n",
    "    print(\"Error fetching data:\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Data Structure\n",
    "\n",
    "Understand the schema, data types, and basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and info\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls\n",
    "null_counts = df_test.isnull().sum()\n",
    "null_pct = (null_counts / len(df_test) * 100).round(2)\n",
    "pd.DataFrame({'Null Count': null_counts, 'Null %': null_pct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore each column\n",
    "for col in df_test.columns:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Type: {df_test[col].dtype}\")\n",
    "    print(f\"Unique values: {df_test[col].nunique()}\")\n",
    "    \n",
    "    if df_test[col].dtype == 'object' and df_test[col].nunique() <= 20:\n",
    "        print(f\"\\nValue counts:\")\n",
    "        print(df_test[col].value_counts())\n",
    "    elif pd.api.types.is_numeric_dtype(df_test[col]):\n",
    "        print(f\"\\nStatistics:\")\n",
    "        print(df_test[col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Design Validation Rules\n",
    "\n",
    "Based on exploration, determine what validation rules to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical fields - extract unique values for validation\n",
    "categorical_fields = ['town', 'flat_type', 'flat_model', 'storey_range']\n",
    "\n",
    "for field in categorical_fields:\n",
    "    if field in df_test.columns:\n",
    "        unique_vals = sorted(df_test[field].dropna().unique())\n",
    "        print(f\"\\n{field}: {len(unique_vals)} unique values\")\n",
    "        print(unique_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range validation\n",
    "if 'month' in df_test.columns:\n",
    "    print(f\"Date range: {df_test['month'].min()} to {df_test['month'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Duplicate Detection\n",
    "\n",
    "Check if duplicates exist and test deduplication logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define composite key (all columns except resale_price)\n",
    "key_columns = [col for col in df_test.columns if col != 'resale_price']\n",
    "print(f\"Composite key columns ({len(key_columns)}): {key_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df_test.duplicated(subset=key_columns, keep=False)\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Found {num_duplicates} duplicate records\")\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(\"\\nSample duplicates:\")\n",
    "    df_test[duplicates].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deduplication - keep higher price\n",
    "df_sorted = df_test.sort_values('resale_price', ascending=False)\n",
    "df_deduped = df_sorted.drop_duplicates(subset=key_columns, keep='first')\n",
    "\n",
    "print(f\"Original: {len(df_test)} records\")\n",
    "print(f\"After dedup: {len(df_deduped)} records\")\n",
    "print(f\"Removed: {len(df_test) - len(df_deduped)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Outlier Detection\n",
    "\n",
    "Test different methods for detecting anomalous prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price distribution\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df_test['resale_price'].hist(bins=50)\n",
    "plt.title('Resale Price Distribution')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_test.boxplot(column='resale_price', by='flat_type', figsize=(14, 5))\n",
    "plt.title('Price by Flat Type')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IQR method\n",
    "Q1 = df_test['resale_price'].quantile(0.25)\n",
    "Q3 = df_test['resale_price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "iqr_outliers = (df_test['resale_price'] < lower_bound) | (df_test['resale_price'] > upper_bound)\n",
    "print(f\"IQR Method: {iqr_outliers.sum()} outliers ({iqr_outliers.sum()/len(df_test)*100:.2f}%)\")\n",
    "print(f\"Bounds: {lower_bound:,.0f} - {upper_bound:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Z-score method\n",
    "z_scores = np.abs((df_test['resale_price'] - df_test['resale_price'].mean()) / df_test['resale_price'].std())\n",
    "z_outliers = z_scores > 3\n",
    "print(f\"Z-Score Method: {z_outliers.sum()} outliers ({z_outliers.sum()/len(df_test)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Transformations\n",
    "\n",
    "### 6.1 Remaining Lease Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test remaining lease calculation\n",
    "def calculate_remaining_lease(lease_commence_date, reference_date=None):\n",
    "    if reference_date is None:\n",
    "        reference_date = datetime.now()\n",
    "    \n",
    "    lease_start = datetime(year=int(lease_commence_date), month=1, day=1)\n",
    "    lease_end = lease_start + relativedelta(years=99)\n",
    "    remaining = relativedelta(lease_end, reference_date)\n",
    "    \n",
    "    years = remaining.years\n",
    "    months = remaining.months\n",
    "    \n",
    "    if years < 0:\n",
    "        return \"0 years 0 months\"\n",
    "    \n",
    "    return f\"{years} years {months} months\"\n",
    "\n",
    "# Test\n",
    "test_cases = [1980, 1990, 2000, 2010]\n",
    "for year in test_cases:\n",
    "    result = calculate_remaining_lease(year)\n",
    "    print(f\"Lease started {year}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataframe\n",
    "if 'lease_commence_date' in df_test.columns:\n",
    "    df_test['remaining_lease'] = df_test['lease_commence_date'].apply(calculate_remaining_lease)\n",
    "    print(df_test[['lease_commence_date', 'remaining_lease']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Resale Identifier Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for identifier creation\n",
    "def extract_block_digits(block):\n",
    "    \"\"\"Extract first 3 digits from block, left-padded\"\"\"\n",
    "    digits_only = re.sub(r'\\D', '', str(block))\n",
    "    if not digits_only:\n",
    "        return \"000\"\n",
    "    return digits_only[:3].zfill(3)\n",
    "\n",
    "def extract_price_digits(average_price):\n",
    "    \"\"\"Extract 1st and 2nd digit from average price\"\"\"\n",
    "    price_str = str(int(average_price))\n",
    "    if len(price_str) >= 2:\n",
    "        return price_str[:2]\n",
    "    else:\n",
    "        return price_str.zfill(2)\n",
    "\n",
    "def extract_month_digits(month_str):\n",
    "    \"\"\"Extract month as 2 digits\"\"\"\n",
    "    try:\n",
    "        date_obj = datetime.strptime(month_str, \"%Y-%m\")\n",
    "        return date_obj.strftime(\"%m\")\n",
    "    except ValueError:\n",
    "        return \"00\"\n",
    "\n",
    "def get_first_char_town(town):\n",
    "    \"\"\"Get first character of town\"\"\"\n",
    "    if not town:\n",
    "        return \"X\"\n",
    "    return str(town).strip().upper()[0]\n",
    "\n",
    "# Test each component\n",
    "print(\"Block tests:\")\n",
    "print(f\"  '19' → '{extract_block_digits('19')}'\")\n",
    "print(f\"  '123A' → '{extract_block_digits('123A')}'\")\n",
    "print(f\"  '5' → '{extract_block_digits('5')}'\")\n",
    "\n",
    "print(\"\\nPrice tests:\")\n",
    "print(f\"  230000 → '{extract_price_digits(230000)}'\")\n",
    "print(f\"  450000 → '{extract_price_digits(450000)}'\")\n",
    "\n",
    "print(\"\\nMonth tests:\")\n",
    "print(f\"  '2012-01' → '{extract_month_digits('2012-01')}'\")\n",
    "print(f\"  '2016-12' → '{extract_month_digits('2016-12')}'\")\n",
    "\n",
    "print(\"\\nTown tests:\")\n",
    "print(f\"  'Ang Mo Kio' → '{get_first_char_town('Ang Mo Kio')}'\")\n",
    "print(f\"  'Bedok' → '{get_first_char_town('Bedok')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average prices by group\n",
    "avg_prices = df_test.groupby(['month', 'town', 'flat_type'])['resale_price'].mean().reset_index()\n",
    "avg_prices.columns = ['month', 'town', 'flat_type', 'avg_resale_price']\n",
    "\n",
    "print(f\"Calculated {len(avg_prices)} average price groups\")\n",
    "avg_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back to main dataframe\n",
    "df_with_avg = df_test.merge(avg_prices, on=['month', 'town', 'flat_type'], how='left')\n",
    "\n",
    "# Create identifiers\n",
    "df_with_avg['resale_identifier'] = df_with_avg.apply(\n",
    "    lambda row: (\n",
    "        \"S\" +\n",
    "        extract_block_digits(row['block']) +\n",
    "        extract_price_digits(row['avg_resale_price']) +\n",
    "        extract_month_digits(row['month']) +\n",
    "        get_first_char_town(row['town'])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"Sample identifiers:\")\n",
    "df_with_avg[['block', 'month', 'town', 'avg_resale_price', 'resale_identifier']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check uniqueness\n",
    "unique_ids = df_with_avg['resale_identifier'].nunique()\n",
    "total_records = len(df_with_avg)\n",
    "print(f\"Unique identifiers: {unique_ids}\")\n",
    "print(f\"Total records: {total_records}\")\n",
    "if unique_ids == total_records:\n",
    "    print(\"✓ All identifiers are unique!\")\n",
    "else:\n",
    "    print(f\"⚠ {total_records - unique_ids} duplicate identifiers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 SHA-256 Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hashing\n",
    "def hash_sha256(identifier):\n",
    "    return hashlib.sha256(identifier.encode('utf-8')).hexdigest()\n",
    "\n",
    "# Test determinism\n",
    "test_id = \"S0192301A\"\n",
    "hash1 = hash_sha256(test_id)\n",
    "hash2 = hash_sha256(test_id)\n",
    "\n",
    "print(f\"Identifier: {test_id}\")\n",
    "print(f\"Hash 1: {hash1}\")\n",
    "print(f\"Hash 2: {hash2}\")\n",
    "print(f\"\\nDeterministic: {hash1 == hash2}\")\n",
    "print(f\"Length: {len(hash1)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply hashing\n",
    "df_with_avg['resale_identifier_hash'] = df_with_avg['resale_identifier'].apply(hash_sha256)\n",
    "\n",
    "print(\"Sample hashes:\")\n",
    "df_with_avg[['resale_identifier', 'resale_identifier_hash']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify hash uniqueness\n",
    "unique_hashes = df_with_avg['resale_identifier_hash'].nunique()\n",
    "print(f\"Unique hashes: {unique_hashes}\")\n",
    "print(f\"Total records: {len(df_with_avg)}\")\n",
    "\n",
    "if unique_hashes == len(df_with_avg):\n",
    "    print(\"✓ Hash uniqueness verified!\")\n",
    "else:\n",
    "    print(f\"⚠ Collision detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Based on this exploration:\n",
    "\n",
    "### Validation Rules to Implement:\n",
    "1. ✅ Categorical validation for town, flat_type, flat_model, storey_range\n",
    "2. ✅ Date range validation\n",
    "3. ✅ Null checks for critical fields\n",
    "4. ✅ Duplicate detection with composite key\n",
    "5. ✅ Outlier detection using IQR and Z-score methods\n",
    "\n",
    "### Transformations to Implement:\n",
    "1. ✅ Remaining lease calculation (99-year assumption)\n",
    "2. ✅ Resale identifier creation (multi-step logic)\n",
    "3. ✅ SHA-256 hashing\n",
    "\n",
    "**Next Step:** Migrate working code to `data_operations/` modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
